---
title: "Lab: gradient methods"
subtitle: "BIOS 731: Advanced Statistical Computing"
author: "Julia Wrobel"
output:
  html_document: 
    code_folding: hide
    toc: true
    toc_float: true
hitheme: tomorrow
highlighter: highlight.js
---

```{r, echo= FALSE, include = FALSE}
library(tidyverse)

knitr::opts_chunk$set(
  echo = FALSE,
  warning = FALSE,
  message = FALSE,
  fig.width = 7,
  fig.height = 5
)

theme_set(theme_bw() + theme(legend.position = "bottom"))
```



# Poisson regression

$$\log(E[Y_i|X_i]) = X_i^T\beta $$

- $Y_i \sim Poisson(\mu_i)$
- $Var(\mu_i) = \mu_i$
- $g'(\mu_i) = \frac{1}{\mu_i}$

$$l(\beta) = \sum_i \left( y_iX_i^T\beta-e^{X_i^T\beta}-\log y_i!\right)$$


- **gradient**: $\frac{\partial l(\beta)}{\partial \beta} = \sum_i(Y_i - e^{X_i^T\beta})X_i$


- **Hessian**: $\frac{\partial^2 l(\beta)}{\partial \beta^2} = -\sum_i e^{X_i^T\beta} X_i^TX_i$



## **Exercise 1**

The function `newton()` below performs Newton's method for the lab example from last week.  Make a new function, `newton_poisson()`, to find MLE estimates for $\beta$ in Poisson regression. Use a simulated dataset with the following parameters:

Simulation study specification:

- simulate from the model $log(E(Y_i = 1|X_i)) = \beta_0 + \beta_1X_i$
  - $\beta_0 = 1$
  - $\beta_1 = 0.3$
  - $X_i \sim N(0, 1)$
  - $n = 100$
  - $nsim = 1$

Try a couple starting parameters for beta, including both good (close to the ground truth) and bad values.


```{r}
newton_poisson = function(beta0 = c(0,0), tol = 1e-6, max_iter = 100) {

  beta_cur = beta0
  beta_history = gradient_vec = matrix(NA, nrow = max_iter, 
                                       ncol = length(beta0))
  for (iter in 1:max_iter) {
    
    # store results
    beta_history[iter,] = beta_cur
    
    # Compute the gradient and hessian
    gradient = as.numeric(t(y-exp(x%*%beta_cur))%*%x)
    
    hessian_ls = as.list(rep(NA, length.out = length(y)))
    for(i in 1:length(y)){
      hessian_ls[[i]] = as.numeric(exp(x[i,]%*%beta_cur)) * tcrossprod(x[i,], x[i,])
    }
    
    hessian <- -1 * Reduce("+", hessian_ls)
    
    gradient_vec[iter,] = gradient
    
    #se
    se = - solve(hessian)
    
    # Check stopping criterion
    if(sqrt(sum(gradient^2)) < tol){
      message("Converged in", iter, "iterations.\n")
      break
    }
    
    # Update the solution
    beta_cur = beta_cur - solve(hessian) %*% gradient
  }
  
  return(list(solution = beta_cur, 
              se = se,
              beta_history = beta_history,
              gradient = gradient_vec,
              converged = (iter < max_iter),
              niter = iter))
}
```

- simulate from the model $log(E(Y_i = 1|X_i)) = \beta_0 + \beta_1X_i$
  - $\beta_0 = 1$
  - $\beta_1 = 0.3$
  - $X_i \sim N(0, 1)$
  - $n = 100$
  - $nsim = 1$

```{r}
set.seed(2343243)
n = 100
beta0 = 1
beta1 = 0.3
x = rnorm(n)
x = cbind(1, x)
lambda = exp(beta0 + beta1 * x)
y = rpois(n, lambda)

beta0 = c(1,0.3)

beta00 = newton_poisson(c(-2, 5))

beta00$solution

mod = glm(y ~x, family = poisson)
coef(mod)
```




## **Exercise 2**

Modify your `newton_poisson()` function to return a 95\% confidence interval for $\beta$.  How do your results compare to `glm()`?

# Logistic regression

This is the beginning of HW 2.

For a given subject in a study, we are interested in modeling $\pi_i = P(Y_i = 1|X_i = x_i)$, where $Y_i \in \{0, 1\}$. The logistic regression model takes the form

<br>

$$\text{logit}(\pi_i) = \log \left(\frac{\pi_i}{1-\pi_i}\right) = \log\left({\frac{P(Y_i = 1|X_i)}{1-P(Y_i = 1|X_i)}}\right) = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + \ldots + \beta_pX_{pi}$$

* $Y_1, Y_2,\ldots, Y_n \sim Bernoulli(\pi)$ 
* PDF is $f(y_i; \pi) = \pi^{y_i}(1-\pi)^{1-y_i}$

## **Exercise 3**

- Derive likelihood, gradient, and Hessian for logistic regression for an arbitrary number of predictors $p$
- What is the Newton's method update for $\beta$ for logistic regression?


- Is logistic regression a convex optimization problem? Why or why not?
